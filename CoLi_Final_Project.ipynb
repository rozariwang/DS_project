{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsV9GEFzakoTuINNYkm20X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rozariwang/DS_project/blob/main/CoLi_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Computational Linguistics 23/24 Final Project** <br>\n",
        "Name: Ho-Hsuan Wang <br>\n",
        "Student Number: 7038925 <br>\n",
        "Date: 22nd March <br>"
      ],
      "metadata": {
        "id": "SMl5PR34Qveh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download and Inspect Datasets"
      ],
      "metadata": {
        "id": "6KikrD098-Ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljsl3PzrUPM_",
        "outputId": "f7ca79e7-3882-465a-b637-d31f0ae14114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install requests conllu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from conllu import parse\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "uDjB0zfOKbCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to UD_Chinese-GSD repository: https://github.com/UniversalDependencies/UD_Chinese-GSD\n",
        "\n",
        "Description: UD_Chinese-GSD is a traditional Chinese Universal Dependencies Treebank annotated and converted by Google."
      ],
      "metadata": {
        "id": "QpMCMsBvMkJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links_to_datasets = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-train.conllu\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-test.conllu\",\n",
        "    \"dev\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-dev.conllu\",\n",
        "}\n",
        "\n",
        "def download_and_parse(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    file_content = response.text\n",
        "    parsed_data = parse(file_content)\n",
        "    return parsed_data\n",
        "\n",
        "# Column headers and descriptions\n",
        "columns_info = {\n",
        "    \"ID\": \"Token ID, integer or decimal for multiword tokens\",\n",
        "    \"FORM\": \"Form or spelling of the word\",\n",
        "    \"LEMMA\": \"Lemma or base form of the word\",\n",
        "    \"UPOS\": \"Universal Part-of-Speech tag\",\n",
        "    \"XPOS\": \"Language-specific part-of-speech tag\",\n",
        "    \"FEATS\": \"Morphological features\",\n",
        "    \"HEAD\": \"Head of the current token in a dependency parse\",\n",
        "    \"DEPREL\": \"Dependency relation to the HEAD\",\n",
        "    \"DEPS\": \"Enhanced dependency graph\",\n",
        "    \"MISC\": \"Any other annotation\"\n",
        "}\n",
        "\n",
        "datasets = {name: download_and_parse(url) for name, url in links_to_datasets.items()}\n",
        "\n",
        "# Print column information\n",
        "print(\"CoNLL-U Format Columns:\")\n",
        "for column, description in columns_info.items():\n",
        "    print(f\"{column}: {description}\")\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# Print dataset information\n",
        "for name, data in datasets.items():\n",
        "    print(f\"Dataset: {name}, Sentences: {len(data)}\")\n",
        "    if data:\n",
        "        first_sentence = data[0]\n",
        "        print(f\"# sent_id = {first_sentence.metadata['sent_id']}\")\n",
        "        print(f\"# text = {first_sentence.metadata['text']}\")\n",
        "        if 'translit' in first_sentence.metadata:\n",
        "            print(f\"# translit = {first_sentence.metadata['translit']}\")\n",
        "        for token in first_sentence:\n",
        "            print(f\"{token['id']}\\t{token['form']}\\t{token['lemma']}\\t{token['upos']}\\t{token['xpos']}\\t{'_'}\\t{token.get('head', '_')}\\t{token['deprel']}\\t{'_'}\\t{'|'.join([f'{k}={v}' for k, v in token['misc'].items()]) if token['misc'] else '_'}\")\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Qnvk5AKlLc",
        "outputId": "83eea16f-98ad-4a3a-f70b-093152b59314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CoNLL-U Format Columns:\n",
            "ID: Token ID, integer or decimal for multiword tokens\n",
            "FORM: Form or spelling of the word\n",
            "LEMMA: Lemma or base form of the word\n",
            "UPOS: Universal Part-of-Speech tag\n",
            "XPOS: Language-specific part-of-speech tag\n",
            "FEATS: Morphological features\n",
            "HEAD: Head of the current token in a dependency parse\n",
            "DEPREL: Dependency relation to the HEAD\n",
            "DEPS: Enhanced dependency graph\n",
            "MISC: Any other annotation\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: train, Sentences: 3997\n",
            "# sent_id = train-s1\n",
            "# text = 看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究底，最後決定的還是自己。\n",
            "# translit = kànshìjiǎndān,zhǐshì'èrxuǎnyīzuòjuézé,dànqíshítāmendàibiǎodeshìnǐzhōuzāodeqīnpénghǎoyou,shìzhegěinǐbùtóngdeyìjiàn,dànzhuīgēnjiūdǐ,zuìhòujuédìngdeháishìzìjǐ.\n",
            "1\t看似\t看似\tVERB\tVV\t_\t5\tadvcl\t_\tSpaceAfter=No|Translit=kànshì|LTranslit=kànshì\n",
            "2\t簡單\t簡單\tADJ\tJJ\t_\t1\txcomp\t_\tSpaceAfter=No|Translit=jiǎndān|LTranslit=jiǎndān\n",
            "3\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "4\t只\t只\tADV\tRB\t_\t5\tadvmod\t_\tSpaceAfter=No|Translit=zhǐ|LTranslit=zhǐ\n",
            "5\t是\t是\tVERB\tVC\t_\t0\troot\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "6\t二\t二\tNUM\tCD\t_\t7\tobl\t_\tSpaceAfter=No|Translit='èr|LTranslit='èr\n",
            "7\t選\t選\tVERB\tVV\t_\t9\tadvcl\t_\tSpaceAfter=No|Translit=xuǎn|LTranslit=xuǎn\n",
            "8\t一\t一\tNUM\tCD\t_\t7\tobj\t_\tSpaceAfter=No|Translit=yī|LTranslit=yī\n",
            "9\t做\t做\tVERB\tVV\t_\t5\txcomp\t_\tSpaceAfter=No|Translit=zuò|LTranslit=zuò\n",
            "10\t決擇\t決擇\tNOUN\tNN\t_\t9\tobj\t_\tSpaceAfter=No|Translit=juézé|LTranslit=juézé\n",
            "11\t，\t，\tPUNCT\t,\t_\t22\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "12\t但\t但\tSCONJ\tRB\t_\t22\tmark\t_\tSpaceAfter=No|Translit=dàn|LTranslit=dàn\n",
            "13\t其實\t其實\tADV\tRB\t_\t22\tadvmod\t_\tSpaceAfter=No|Translit=qíshí|LTranslit=qíshí\n",
            "14\t他們\t他\tPRON\tPRP\t_\t15\tnsubj\t_\tSpaceAfter=No|Translit=tāmen|LTranslit=tā\n",
            "15\t代表\t代表\tVERB\tVV\t_\t22\tcsubj\t_\tSpaceAfter=No|Translit=dàibiǎo|LTranslit=dàibiǎo\n",
            "16\t的\t的\tSCONJ\tDEC\t_\t15\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "17\t是\t是\tAUX\tVC\t_\t22\tcop\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "18\t你\t你\tPRON\tPRP\t_\t19\tnsubj\t_\tSpaceAfter=No|Translit=nǐ|LTranslit=nǐ\n",
            "19\t周遭\t周遭\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No|Translit=zhōuzāo|LTranslit=zhōuzāo\n",
            "20\t的\t的\tPART\tDEC\t_\t19\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "21\t親朋\t親朋\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No|Translit=qīnpéng|LTranslit=qīnpéng\n",
            "22\t好友\t好友\tNOUN\tNN\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=hǎoyou|LTranslit=hǎoyou\n",
            "23\t，\t，\tPUNCT\t,\t_\t24\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "24\t試\t試\tVERB\tVV\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "25\t著\t著\tAUX\tAS\t_\t24\taux\t_\tSpaceAfter=No|Translit=zhe|LTranslit=zhe\n",
            "26\t給\t給\tVERB\tVV\t_\t24\txcomp\t_\tSpaceAfter=No|Translit=gěi|LTranslit=gěi\n",
            "27\t你\t你\tPRON\tPRP\t_\t26\tiobj\t_\tSpaceAfter=No|Translit=nǐ|LTranslit=nǐ\n",
            "28\t不同\t不同\tADJ\tJJ\t_\t30\tamod\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "29\t的\t的\tSCONJ\tDEC\t_\t28\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "30\t意見\t意見\tNOUN\tNN\t_\t26\tobj\t_\tSpaceAfter=No|Translit=yìjiàn|LTranslit=yìjiàn\n",
            "31\t，\t，\tPUNCT\t,\t_\t39\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "32\t但\t但\tSCONJ\tRB\t_\t39\tmark\t_\tSpaceAfter=No|Translit=dàn|LTranslit=dàn\n",
            "33\t追根究底\t追根究底\tVERB\tVV\t_\t39\tacl\t_\tSpaceAfter=No|Translit=zhuīgēnjiūdǐ|LTranslit=zhuīgēnjiūdǐ\n",
            "34\t，\t，\tPUNCT\t,\t_\t33\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "35\t最後\t最後\tNOUN\tNN\t_\t36\tnmod:tmod\t_\tSpaceAfter=No|Translit=zuìhòu|LTranslit=zuìhòu\n",
            "36\t決定\t決定\tVERB\tVV\t_\t39\tcsubj\t_\tSpaceAfter=No|Translit=juédìng|LTranslit=juédìng\n",
            "37\t的\t的\tSCONJ\tDEC\t_\t36\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "38\t還是\t是\tAUX\tVC\t_\t39\tcop\t_\tSpaceAfter=No|Translit=háishì|LTranslit=shì\n",
            "39\t自己\t自己\tPRON\tPRD\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=zìjǐ|LTranslit=zìjǐ\n",
            "40\t。\t。\tPUNCT\t.\t_\t5\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: test, Sentences: 500\n",
            "# sent_id = test-s1\n",
            "# text = 然而，這樣的處理也衍生了一些問題。\n",
            "# translit = rán'ér,zhèyàngdechùlǐyěyǎnshēngleyīxiēwèntí.\n",
            "1\t然而\t然而\tSCONJ\tRB\t_\t7\tmark\t_\tSpaceAfter=No|Translit=rán'ér|LTranslit=rán'ér\n",
            "2\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "3\t這樣\t這樣\tPRON\tPRD\t_\t5\tdet\t_\tSpaceAfter=No|Translit=zhèyàng|LTranslit=zhèyàng\n",
            "4\t的\t的\tPART\tDEC\t_\t3\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "5\t處理\t處理\tNOUN\tNN\t_\t7\tnsubj\t_\tSpaceAfter=No|Translit=chùlǐ|LTranslit=chùlǐ\n",
            "6\t也\t也\tSCONJ\tRB\t_\t7\tmark\t_\tSpaceAfter=No|Translit=yě|LTranslit=yě\n",
            "7\t衍生\t衍生\tVERB\tVV\t_\t0\troot\t_\tSpaceAfter=No|Translit=yǎnshēng|LTranslit=yǎnshēng\n",
            "8\t了\t了\tAUX\tAS\t_\t7\taux\t_\tSpaceAfter=No|Translit=le|LTranslit=le\n",
            "9\t一些\t一些\tADJ\tJJ\t_\t10\tamod\t_\tSpaceAfter=No|Translit=yīxiē|LTranslit=yīxiē\n",
            "10\t問題\t問題\tNOUN\tNN\t_\t7\tobj\t_\tSpaceAfter=No|Translit=wèntí|LTranslit=wèntí\n",
            "11\t。\t。\tPUNCT\t.\t_\t7\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: dev, Sentences: 500\n",
            "# sent_id = dev-s1\n",
            "# text = 同樣，施力的大小不同，引起的加速度不同，最終的結果也不一樣，亦可以從向量的加成性來看。\n",
            "# translit = tóngyàng,shīlìdedàxiǎobùtóng,yǐnqǐdejiāsùdùbùtóng,zuìzhōngdejiéguǒyěbùyīyàng,yìkěyǐcóngxiàngliàngdejiāchéngxìngláikàn.\n",
            "1\t同樣\t同樣\tADV\tRB\t_\t12\tadvmod\t_\tSpaceAfter=No|Translit=tóngyàng|LTranslit=tóngyàng\n",
            "2\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "3\t施力\t施力\tVERB\tVV\t_\t5\tacl:relcl\t_\tSpaceAfter=No|Translit=shīlì|LTranslit=shīlì\n",
            "4\t的\t的\tSCONJ\tDEC\t_\t3\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "5\t大小\t大小\tNOUN\tNN\t_\t6\tnsubj\t_\tSpaceAfter=No|Translit=dàxiǎo|LTranslit=dàxiǎo\n",
            "6\t不同\t不同\tADJ\tJJ\t_\t8\tcsubj\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "7\t，\t，\tPUNCT\t,\t_\t6\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "8\t引起\t引起\tVERB\tVV\t_\t11\tacl:relcl\t_\tSpaceAfter=No|Translit=yǐnqǐ|LTranslit=yǐnqǐ\n",
            "9\t的\t的\tSCONJ\tDEC\t_\t8\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "10\t加速\t加速\tVERB\tVV\t_\t11\tcompound\t_\tSpaceAfter=No|Translit=jiāsù|LTranslit=jiāsù\n",
            "11\t度\t度\tPART\tSFN\t_\t12\tnsubj\t_\tSpaceAfter=No|Translit=dù|LTranslit=dù\n",
            "12\t不同\t不同\tADJ\tJJ\t_\t0\troot\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "13\t，\t，\tPUNCT\t,\t_\t29\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "14\t最終\t最終\tNOUN\tNN\t_\t16\tnmod\t_\tSpaceAfter=No|Translit=zuìzhōng|LTranslit=zuìzhōng\n",
            "15\t的\t的\tPART\tDEC\t_\t14\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "16\t結果\t結果\tNOUN\tNN\t_\t29\tnsubj\t_\tSpaceAfter=No|Translit=jiéguǒ|LTranslit=jiéguǒ\n",
            "17\t也\t也\tSCONJ\tRB\t_\t19\tmark\t_\tSpaceAfter=No|Translit=yě|LTranslit=yě\n",
            "18\t不\t不\tADV\tRB\t_\t19\tadvmod\t_\tSpaceAfter=No|Translit=bù|LTranslit=bù\n",
            "19\t一樣\t一樣\tADJ\tJJ\t_\t29\tadvcl\t_\tSpaceAfter=No|Translit=yīyàng|LTranslit=yīyàng\n",
            "20\t，\t，\tPUNCT\t,\t_\t19\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "21\t亦\t亦\tSCONJ\tRB\t_\t29\tmark\t_\tSpaceAfter=No|Translit=yì|LTranslit=yì\n",
            "22\t可以\t可以\tAUX\tMD\t_\t29\taux\t_\tSpaceAfter=No|Translit=kěyǐ|LTranslit=kěyǐ\n",
            "23\t從\t從\tADP\tIN\t_\t27\tcase\t_\tSpaceAfter=No|Translit=cóng|LTranslit=cóng\n",
            "24\t向量\t向量\tNOUN\tNN\t_\t27\tnmod\t_\tSpaceAfter=No|Translit=xiàngliàng|LTranslit=xiàngliàng\n",
            "25\t的\t的\tPART\tDEC\t_\t24\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "26\t加成\t加成\tVERB\tVV\t_\t27\tcompound\t_\tSpaceAfter=No|Translit=jiāchéng|LTranslit=jiāchéng\n",
            "27\t性\t性\tPART\tSFN\t_\t29\tobl\t_\tSpaceAfter=No|Translit=xìng|LTranslit=xìng\n",
            "28\t來\t來\tSCONJ\tRB\t_\t29\tmark\t_\tSpaceAfter=No|Translit=lái|LTranslit=lái\n",
            "29\t看\t看\tVERB\tVV\t_\t12\tparataxis\t_\tSpaceAfter=No|Translit=kàn|LTranslit=kàn\n",
            "30\t。\t。\tPUNCT\t.\t_\t12\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Preprocess the data from the CoNLL-U files and prepares it for training the XLM-RoBERTa model. <br>\n",
        "\n",
        "- Tokenize and align the data (align the POS tags with the subword tokens) using the XLMRobertaTokenizerFast since XLM-RoBERTa uses subword tokenization."
      ],
      "metadata": {
        "id": "4c1NFE-ak_F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMrTTIUqk-EP",
        "outputId": "acb2feec-0075-4372-a92f-286cc903675d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import requests\n",
        "from conllu import parse\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "4n3vtYallaVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"xlm-roberta-base\"\n",
        "\n",
        "dataset = download_and_parse(links_to_datasets[\"train\"])\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "def align_tokens_and_labels(sentence, label_field='upos'):\n",
        "    tokens = [token['form'] for token in sentence]  # Original words from the sentence\n",
        "    labels = [token[label_field] for token in sentence]  # Corresponding labels\n",
        "\n",
        "    # Tokenize the sentence, indicating that the input is pre-tokenized\n",
        "    tokenized_input = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
        "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
        "\n",
        "    aligned_labels = []\n",
        "    # Iterate over word_ids to assign labels correctly\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            aligned_labels.append(-100)  # Assign -100 to special tokens\n",
        "        else:\n",
        "            # If the token starts with \"▁\" and is not just \"▁\", assign the label\n",
        "            if tokenized_input.tokens()[i].startswith(\"▁\") and len(tokenized_input.tokens()[i]) > 1:\n",
        "                aligned_labels.append(labels[word_idx])\n",
        "            elif not tokenized_input.tokens()[i].startswith(\"▁\"):\n",
        "                # Assign the label to the actual word token (not the underscore)\n",
        "                aligned_labels.append(labels[word_idx])\n",
        "            else:\n",
        "                # If it's just \"▁\", assign -100\n",
        "                aligned_labels.append(-100)\n",
        "\n",
        "    return tokenized_input, aligned_labels\n",
        "\n",
        "\n",
        "##### Check if the tokenise and align functions correctly #####\n",
        "# Align tokens and labels for the first sentence as an example\n",
        "first_sentence = dataset[0]\n",
        "tokenized_input, aligned_labels = align_tokens_and_labels(first_sentence)\n",
        "\n",
        "print(first_sentence)\n",
        "print(\"Tokenized input:\", tokenized_input)\n",
        "print(\"Aligned labels:\", aligned_labels)\n",
        "\n",
        "# Convert token IDs back to tokens (words/subwords)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'][0])\n",
        "\n",
        "# Print each token with its aligned label\n",
        "for token, label in zip(tokens, aligned_labels):\n",
        "    print(f\"{token} --> {label}\\n\")\n",
        "################################################################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9kIjYD2ldIx",
        "outputId": "2fda9506-2b03-4549-85aa-24cd88a748b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TokenList<看似, 簡單, ，, 只, 是, 二, 選, 一, 做, 決擇, ，, 但, 其實, 他們, 代表, 的, 是, 你, 周遭, 的, 親朋, 好友, ，, 試, 著, 給, 你, 不同, 的, 意見, ，, 但, 追根究底, ，, 最後, 決定, 的, 還是, 自己, 。, metadata={sent_id: \"train-s1\", text: \"看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究底，最後決定的還是自己。\", translit: \"kànshìjiǎndān,zhǐshì'èrxuǎnyīzuòjuézé,dànqíshítāmendàibiǎodeshìnǐzhōuzāodeqīnpénghǎoyou,shìzhegěinǐbùtóngdeyìjiàn,dànzhuīgēnjiūdǐ,zuìhòujuédìngdeháishìzìjǐ.\"}>\n",
            "Tokenized input: {'input_ids': tensor([[     0,      6, 113875,      6,  32564,      6,      4,      6,   5344,\n",
            "              6,    354,  87744,      6,   6995,  45690,      6,   2213,      6,\n",
            "          33808, 235211,      6,      4,  53072,      6,  16827,      6,   8056,\n",
            "              6,   6959,      6,     43,      6,    354,  73675,      6,   6271,\n",
            "          44162,      6,     43,      6,  11638, 182529,      6,  81070,      6,\n",
            "              4,      6,  12324,      6,   3094,      6,   5862,  73675,      6,\n",
            "           5714,      6,     43,      6,  31505,      6,      4,  53072,      6,\n",
            "          17411,  12442,  78382,  11298,      6,      4,      6,  15394,      6,\n",
            "          17755,      6,     43,      6,  11097,      6,   1652,      6,     30,\n",
            "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Aligned labels: [-100, -100, 'VERB', -100, 'ADJ', -100, 'PUNCT', -100, 'ADV', -100, 'VERB', 'NUM', -100, 'VERB', 'NUM', -100, 'VERB', -100, 'NOUN', 'NOUN', -100, 'PUNCT', 'SCONJ', -100, 'ADV', -100, 'PRON', -100, 'VERB', -100, 'SCONJ', -100, 'AUX', 'PRON', -100, 'NOUN', 'NOUN', -100, 'PART', -100, 'NOUN', 'NOUN', -100, 'NOUN', -100, 'PUNCT', -100, 'VERB', -100, 'AUX', -100, 'VERB', 'PRON', -100, 'ADJ', -100, 'SCONJ', -100, 'NOUN', -100, 'PUNCT', 'SCONJ', -100, 'VERB', 'VERB', 'VERB', 'VERB', -100, 'PUNCT', -100, 'NOUN', -100, 'VERB', -100, 'SCONJ', -100, 'AUX', -100, 'PRON', -100, 'PUNCT', -100]\n",
            "<s> --> -100\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "看似 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "簡單 --> ADJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "只 --> ADV\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "是 --> VERB\n",
            "\n",
            "▁二 --> NUM\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "選 --> VERB\n",
            "\n",
            "▁一 --> NUM\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "做 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "決 --> NOUN\n",
            "\n",
            "擇 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁但 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "其實 --> ADV\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "他們 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "代表 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "是 --> AUX\n",
            "\n",
            "▁你 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "周 --> NOUN\n",
            "\n",
            "遭 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> PART\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "親 --> NOUN\n",
            "\n",
            "朋 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "好友 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "試 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "著 --> AUX\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "給 --> VERB\n",
            "\n",
            "▁你 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "不同 --> ADJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "意見 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁但 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "追 --> VERB\n",
            "\n",
            "根 --> VERB\n",
            "\n",
            "究 --> VERB\n",
            "\n",
            "底 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "最後 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "決定 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "還是 --> AUX\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "自己 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "。 --> PUNCT\n",
            "\n",
            "</s> --> -100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A class that handles the tokenization and alignment of labels\n",
        "class POSDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, label_to_id):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_to_id = label_to_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def encode_labels(self, labels):\n",
        "        # Convert text labels to numerical IDs\n",
        "        return [self.label_to_id[label] if label in self.label_to_id else self.label_to_id[\"O\"] for label in labels]\n",
        "\n",
        "    def align_tokens_and_labels(self, tokens, labels):\n",
        "      # Tokenize the input, indicating that the input is pre-tokenized\n",
        "      tokenized_input = self.tokenizer(tokens, is_split_into_words=True, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "      word_ids = tokenized_input.word_ids(batch_index=0)\n",
        "\n",
        "      # Initialize the labels to -100 for each token position\n",
        "      aligned_labels = [-100] * 512  # Initialize with -100 for padding tokens\n",
        "\n",
        "      # Align labels with tokens\n",
        "      for i, word_idx in enumerate(word_ids):\n",
        "          if word_idx is not None:\n",
        "              # If the token starts with \"▁\" and is not just \"▁\", or if it doesn't start with \"▁\"\n",
        "              if (tokenized_input.tokens()[i].startswith(\"▁\") and len(tokenized_input.tokens()[i]) > 1) or not tokenized_input.tokens()[i].startswith(\"▁\"):\n",
        "                  # Check if the label exists in the mapping before accessing\n",
        "                  if labels[word_idx] in self.label_to_id:\n",
        "                      aligned_labels[i] = self.label_to_id[labels[word_idx]]\n",
        "                  else:\n",
        "                    # If the label doesn't exist in the mapping, assign a default value (e.g., -100)\n",
        "                    aligned_labels[i] = -100\n",
        "              else:\n",
        "                # If the token is just \"▁\", assign -100\n",
        "                aligned_labels[i] = -100\n",
        "\n",
        "\n",
        "      tokenized_input['labels'] = torch.tensor(aligned_labels, dtype=torch.long)\n",
        "      return tokenized_input\n",
        "\n",
        "    '''\n",
        "    def __getitem__(self, idx):\n",
        "       item = self.data[idx]\n",
        "       tokens = item['tokens']\n",
        "       labels = self.encode_labels(item['labels'])\n",
        "       tokenized_inputs = self.align_tokens_and_labels(tokens, labels)\n",
        "       # Make sure the extraction of input_ids and attention_mask is correct\n",
        "       input_ids = tokenized_input['input_ids'].squeeze()  # This should result in a shape of [512]\n",
        "       attention_mask = tokenized_input['attention_mask'].squeeze()  # This should result in a shape of [512]\n",
        "       labels = tokenized_input['labels']  # This should already be [512]\n",
        "\n",
        "       # Ensure they are all tensors before returning\n",
        "       return {\n",
        "           'input_ids': input_ids,\n",
        "           'attention_mask': attention_mask,\n",
        "           'labels': labels\n",
        "       }\n",
        "       '''\n",
        "    def __getitem__(self, idx):\n",
        "      item = self.data[idx]\n",
        "      tokens = item['tokens']\n",
        "      # Ensure labels are encoded to numerical IDs before passing\n",
        "      labels = self.encode_labels(item['labels'])\n",
        "\n",
        "      # Get the tokenized inputs and aligned labels\n",
        "      tokenized_input = self.align_tokens_and_labels(tokens, labels)\n",
        "\n",
        "      # Explicitly extract input_ids, attention_mask, and labels\n",
        "      input_ids = tokenized_input['input_ids'].squeeze(0) if tokenized_input['input_ids'].size(0) == 1 else tokenized_input['input_ids']  # Ensure this is the correct shape\n",
        "      attention_mask = tokenized_input['attention_mask'].squeeze(0) if tokenized_input['attention_mask'].size(0) == 1 else tokenized_input['attention_mask'] # Ensure this is the correct shape\n",
        "\n",
        "      # Ensure labels are correctly shaped\n",
        "      labels = tokenized_input['labels']\n",
        "      # Verify the shapes\n",
        "      assert input_ids.size(0) == 512 and attention_mask.size(0) == 512, \"Sequence length should be 512\"\n",
        "\n",
        "\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "      }\n"
      ],
      "metadata": {
        "id": "t0D4db2mIfOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's assume your labels are stored in a 'labels' field in your dataset items\n",
        "unique_labels = set(label for item in train_data for label in item['labels'])\n",
        "label_to_id_map = {label: id for id, label in enumerate(unique_labels)}\n",
        "\n",
        "# Now you can use label_to_id_map to create your dataset\n",
        "test_dataset = POSDataset(data=train_data, tokenizer=tokenizer, label_to_id=label_to_id_map)\n",
        "print(len(test_dataset))  # Should now work correctly\n",
        "\n",
        "\n",
        "for batch in test_dataset:\n",
        "    print(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
        "    print(f\"Batch attention_mask shape: {batch['attention_mask'].shape}\")\n",
        "    print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
        "    break  # Just inspect the first batch and break\n",
        "\n",
        "for i, item in enumerate(test_dataset):\n",
        "    # This will print the details for each item\n",
        "    if i == 1:  # Just print details for the first item and break\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYNFogyV9Euk",
        "outputId": "508e8d86-9c72-45dc-fdd3-d29cb1e10d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3997\n",
            "Batch input_ids shape: torch.Size([512])\n",
            "Batch attention_mask shape: torch.Size([512])\n",
            "Batch labels shape: torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]\n",
        "print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
        "print(f\"Attention mask length: {len(sample['attention_mask'])}\")\n",
        "print(f\"Labels length: {len(sample['labels'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teF6mXQ2G247",
        "outputId": "d4b32859-383d-4a94-98ff-90ba7f0b37d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs length: 1\n",
            "Attention mask length: 1\n",
            "Labels length: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_dataset(data):\n",
        "    sentences = []\n",
        "    for sentence in data:\n",
        "        tokens = [token['form'] for token in sentence]\n",
        "        labels = [token['upos'] for token in sentence]\n",
        "        sentences.append({'tokens': tokens, 'labels': labels})\n",
        "    return sentences\n",
        "\n",
        "train_data = parse_dataset(download_and_parse(links_to_datasets[\"train\"]))\n",
        "test_data = parse_dataset(download_and_parse(links_to_datasets[\"test\"]))\n",
        "\n",
        "# Convert POS tags to unique IDs\n",
        "unique_labels = set(label for sentence in (train_data + test_data) for label in sentence['labels'])\n",
        "label_to_id = {label: id for id, label in enumerate(unique_labels)}\n",
        "print(\"Unique Labels: \", unique_labels)\n",
        "print(\"Number of Unique Labels: \", len(unique_labels))\n",
        "\n",
        "# Initialize the datasets\n",
        "train_dataset = POSDataset(train_data, tokenizer, label_to_id)\n",
        "test_dataset = POSDataset(test_data, tokenizer, label_to_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duLxnYLRKX41",
        "outputId": "bec2998c-587e-4cdf-c47f-419dfd5d7970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels:  {'NUM', 'SYM', 'ADP', 'ADJ', 'VERB', 'NOUN', 'PART', 'PROPN', 'PUNCT', 'SCONJ', 'X', 'CCONJ', 'PRON', 'AUX', 'ADV', 'DET'}\n",
            "Number of Unique Labels:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training the Model"
      ],
      "metadata": {
        "id": "VTavOex9GV-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Tk6izaOHIRe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tER003IeNPlX",
        "outputId": "dfcc9cb9-e867-4901-8351-5f4a3e0a6d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "print(accelerate.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdx1PIwVN6m1",
        "outputId": "09ff5a46-c354-4e9a-b96f-37c49485a74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # !pip install transformers[torch] accelerate -U\n",
        "\n"
      ],
      "metadata": {
        "id": "oRv9UZbrN8b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLMRobertaForTokenClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p.predictions, p.label_ids\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Flatten both the predictions and labels\n",
        "    true_labels = labels.flatten()\n",
        "    pred_labels = predictions.flatten()\n",
        "\n",
        "    # Exclude ignored index (specifically -100)\n",
        "    true_labels = true_labels[true_labels != -100]\n",
        "    pred_labels = pred_labels[pred_labels != -100]\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='macro')\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# Ensure that training_args is defined as per your requirements\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\"      # evaluation is done at the end of each epoch\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,  # Use test_dataset for evaluation\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Visualize Training and Validation Loss\n",
        "training_loss = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
        "validation_loss = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(training_loss, label='Training loss')\n",
        "plt.plot(validation_loss, label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzMjw1_GY6k",
        "outputId": "84b8faf3-8be3-46a4-d7ad-29d802d4e0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    }
  ]
}