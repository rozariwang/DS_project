{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMknyTnY7SYFz+elrAKNoSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rozariwang/DS_project/blob/main/CoLi_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Computational Linguistics 23/24 Final Project** <br>\n",
        "Name: Ho-Hsuan Wang <br>\n",
        "Student Number: 7038925 <br>\n",
        "Date: 22nd March <br>"
      ],
      "metadata": {
        "id": "SMl5PR34Qveh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download and Inspect Datasets"
      ],
      "metadata": {
        "id": "6KikrD098-Ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljsl3PzrUPM_",
        "outputId": "85ee277a-bda4-4966-f9c5-9cb17a267e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install requests conllu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from conllu import parse\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "uDjB0zfOKbCi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to UD_Chinese-GSD repository: https://github.com/UniversalDependencies/UD_Chinese-GSD\n",
        "\n",
        "Description: UD_Chinese-GSD is a traditional Chinese Universal Dependencies Treebank annotated and converted by Google."
      ],
      "metadata": {
        "id": "QpMCMsBvMkJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links_to_datasets = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-train.conllu\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-test.conllu\",\n",
        "    \"dev\": \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-dev.conllu\",\n",
        "}\n",
        "\n",
        "def download_and_parse(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    file_content = response.text\n",
        "    parsed_data = parse(file_content)\n",
        "    return parsed_data\n",
        "\n",
        "# Column headers and descriptions\n",
        "columns_info = {\n",
        "    \"ID\": \"Token ID, integer or decimal for multiword tokens\",\n",
        "    \"FORM\": \"Form or spelling of the word\",\n",
        "    \"LEMMA\": \"Lemma or base form of the word\",\n",
        "    \"UPOS\": \"Universal Part-of-Speech tag\",\n",
        "    \"XPOS\": \"Language-specific part-of-speech tag\",\n",
        "    \"FEATS\": \"Morphological features\",\n",
        "    \"HEAD\": \"Head of the current token in a dependency parse\",\n",
        "    \"DEPREL\": \"Dependency relation to the HEAD\",\n",
        "    \"DEPS\": \"Enhanced dependency graph\",\n",
        "    \"MISC\": \"Any other annotation\"\n",
        "}\n",
        "\n",
        "datasets = {name: download_and_parse(url) for name, url in links_to_datasets.items()}\n",
        "\n",
        "# Print column information\n",
        "print(\"CoNLL-U Format Columns:\")\n",
        "for column, description in columns_info.items():\n",
        "    print(f\"{column}: {description}\")\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# Print dataset information\n",
        "for name, data in datasets.items():\n",
        "    print(f\"Dataset: {name}, Sentences: {len(data)}\")\n",
        "    if data:\n",
        "        first_sentence = data[0]\n",
        "        print(f\"# sent_id = {first_sentence.metadata['sent_id']}\")\n",
        "        print(f\"# text = {first_sentence.metadata['text']}\")\n",
        "        if 'translit' in first_sentence.metadata:\n",
        "            print(f\"# translit = {first_sentence.metadata['translit']}\")\n",
        "        for token in first_sentence:\n",
        "            print(f\"{token['id']}\\t{token['form']}\\t{token['lemma']}\\t{token['upos']}\\t{token['xpos']}\\t{'_'}\\t{token.get('head', '_')}\\t{token['deprel']}\\t{'_'}\\t{'|'.join([f'{k}={v}' for k, v in token['misc'].items()]) if token['misc'] else '_'}\")\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Qnvk5AKlLc",
        "outputId": "8c971ce1-6ad2-4cbb-a7e8-ade63824012a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CoNLL-U Format Columns:\n",
            "ID: Token ID, integer or decimal for multiword tokens\n",
            "FORM: Form or spelling of the word\n",
            "LEMMA: Lemma or base form of the word\n",
            "UPOS: Universal Part-of-Speech tag\n",
            "XPOS: Language-specific part-of-speech tag\n",
            "FEATS: Morphological features\n",
            "HEAD: Head of the current token in a dependency parse\n",
            "DEPREL: Dependency relation to the HEAD\n",
            "DEPS: Enhanced dependency graph\n",
            "MISC: Any other annotation\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: train, Sentences: 3997\n",
            "# sent_id = train-s1\n",
            "# text = 看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究底，最後決定的還是自己。\n",
            "# translit = kànshìjiǎndān,zhǐshì'èrxuǎnyīzuòjuézé,dànqíshítāmendàibiǎodeshìnǐzhōuzāodeqīnpénghǎoyou,shìzhegěinǐbùtóngdeyìjiàn,dànzhuīgēnjiūdǐ,zuìhòujuédìngdeháishìzìjǐ.\n",
            "1\t看似\t看似\tVERB\tVV\t_\t5\tadvcl\t_\tSpaceAfter=No|Translit=kànshì|LTranslit=kànshì\n",
            "2\t簡單\t簡單\tADJ\tJJ\t_\t1\txcomp\t_\tSpaceAfter=No|Translit=jiǎndān|LTranslit=jiǎndān\n",
            "3\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "4\t只\t只\tADV\tRB\t_\t5\tadvmod\t_\tSpaceAfter=No|Translit=zhǐ|LTranslit=zhǐ\n",
            "5\t是\t是\tVERB\tVC\t_\t0\troot\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "6\t二\t二\tNUM\tCD\t_\t7\tobl\t_\tSpaceAfter=No|Translit='èr|LTranslit='èr\n",
            "7\t選\t選\tVERB\tVV\t_\t9\tadvcl\t_\tSpaceAfter=No|Translit=xuǎn|LTranslit=xuǎn\n",
            "8\t一\t一\tNUM\tCD\t_\t7\tobj\t_\tSpaceAfter=No|Translit=yī|LTranslit=yī\n",
            "9\t做\t做\tVERB\tVV\t_\t5\txcomp\t_\tSpaceAfter=No|Translit=zuò|LTranslit=zuò\n",
            "10\t決擇\t決擇\tNOUN\tNN\t_\t9\tobj\t_\tSpaceAfter=No|Translit=juézé|LTranslit=juézé\n",
            "11\t，\t，\tPUNCT\t,\t_\t22\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "12\t但\t但\tSCONJ\tRB\t_\t22\tmark\t_\tSpaceAfter=No|Translit=dàn|LTranslit=dàn\n",
            "13\t其實\t其實\tADV\tRB\t_\t22\tadvmod\t_\tSpaceAfter=No|Translit=qíshí|LTranslit=qíshí\n",
            "14\t他們\t他\tPRON\tPRP\t_\t15\tnsubj\t_\tSpaceAfter=No|Translit=tāmen|LTranslit=tā\n",
            "15\t代表\t代表\tVERB\tVV\t_\t22\tcsubj\t_\tSpaceAfter=No|Translit=dàibiǎo|LTranslit=dàibiǎo\n",
            "16\t的\t的\tSCONJ\tDEC\t_\t15\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "17\t是\t是\tAUX\tVC\t_\t22\tcop\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "18\t你\t你\tPRON\tPRP\t_\t19\tnsubj\t_\tSpaceAfter=No|Translit=nǐ|LTranslit=nǐ\n",
            "19\t周遭\t周遭\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No|Translit=zhōuzāo|LTranslit=zhōuzāo\n",
            "20\t的\t的\tPART\tDEC\t_\t19\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "21\t親朋\t親朋\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No|Translit=qīnpéng|LTranslit=qīnpéng\n",
            "22\t好友\t好友\tNOUN\tNN\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=hǎoyou|LTranslit=hǎoyou\n",
            "23\t，\t，\tPUNCT\t,\t_\t24\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "24\t試\t試\tVERB\tVV\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=shì|LTranslit=shì\n",
            "25\t著\t著\tAUX\tAS\t_\t24\taux\t_\tSpaceAfter=No|Translit=zhe|LTranslit=zhe\n",
            "26\t給\t給\tVERB\tVV\t_\t24\txcomp\t_\tSpaceAfter=No|Translit=gěi|LTranslit=gěi\n",
            "27\t你\t你\tPRON\tPRP\t_\t26\tiobj\t_\tSpaceAfter=No|Translit=nǐ|LTranslit=nǐ\n",
            "28\t不同\t不同\tADJ\tJJ\t_\t30\tamod\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "29\t的\t的\tSCONJ\tDEC\t_\t28\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "30\t意見\t意見\tNOUN\tNN\t_\t26\tobj\t_\tSpaceAfter=No|Translit=yìjiàn|LTranslit=yìjiàn\n",
            "31\t，\t，\tPUNCT\t,\t_\t39\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "32\t但\t但\tSCONJ\tRB\t_\t39\tmark\t_\tSpaceAfter=No|Translit=dàn|LTranslit=dàn\n",
            "33\t追根究底\t追根究底\tVERB\tVV\t_\t39\tacl\t_\tSpaceAfter=No|Translit=zhuīgēnjiūdǐ|LTranslit=zhuīgēnjiūdǐ\n",
            "34\t，\t，\tPUNCT\t,\t_\t33\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "35\t最後\t最後\tNOUN\tNN\t_\t36\tnmod:tmod\t_\tSpaceAfter=No|Translit=zuìhòu|LTranslit=zuìhòu\n",
            "36\t決定\t決定\tVERB\tVV\t_\t39\tcsubj\t_\tSpaceAfter=No|Translit=juédìng|LTranslit=juédìng\n",
            "37\t的\t的\tSCONJ\tDEC\t_\t36\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "38\t還是\t是\tAUX\tVC\t_\t39\tcop\t_\tSpaceAfter=No|Translit=háishì|LTranslit=shì\n",
            "39\t自己\t自己\tPRON\tPRD\t_\t5\tparataxis\t_\tSpaceAfter=No|Translit=zìjǐ|LTranslit=zìjǐ\n",
            "40\t。\t。\tPUNCT\t.\t_\t5\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: test, Sentences: 500\n",
            "# sent_id = test-s1\n",
            "# text = 然而，這樣的處理也衍生了一些問題。\n",
            "# translit = rán'ér,zhèyàngdechùlǐyěyǎnshēngleyīxiēwèntí.\n",
            "1\t然而\t然而\tSCONJ\tRB\t_\t7\tmark\t_\tSpaceAfter=No|Translit=rán'ér|LTranslit=rán'ér\n",
            "2\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "3\t這樣\t這樣\tPRON\tPRD\t_\t5\tdet\t_\tSpaceAfter=No|Translit=zhèyàng|LTranslit=zhèyàng\n",
            "4\t的\t的\tPART\tDEC\t_\t3\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "5\t處理\t處理\tNOUN\tNN\t_\t7\tnsubj\t_\tSpaceAfter=No|Translit=chùlǐ|LTranslit=chùlǐ\n",
            "6\t也\t也\tSCONJ\tRB\t_\t7\tmark\t_\tSpaceAfter=No|Translit=yě|LTranslit=yě\n",
            "7\t衍生\t衍生\tVERB\tVV\t_\t0\troot\t_\tSpaceAfter=No|Translit=yǎnshēng|LTranslit=yǎnshēng\n",
            "8\t了\t了\tAUX\tAS\t_\t7\taux\t_\tSpaceAfter=No|Translit=le|LTranslit=le\n",
            "9\t一些\t一些\tADJ\tJJ\t_\t10\tamod\t_\tSpaceAfter=No|Translit=yīxiē|LTranslit=yīxiē\n",
            "10\t問題\t問題\tNOUN\tNN\t_\t7\tobj\t_\tSpaceAfter=No|Translit=wèntí|LTranslit=wèntí\n",
            "11\t。\t。\tPUNCT\t.\t_\t7\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n",
            "Dataset: dev, Sentences: 500\n",
            "# sent_id = dev-s1\n",
            "# text = 同樣，施力的大小不同，引起的加速度不同，最終的結果也不一樣，亦可以從向量的加成性來看。\n",
            "# translit = tóngyàng,shīlìdedàxiǎobùtóng,yǐnqǐdejiāsùdùbùtóng,zuìzhōngdejiéguǒyěbùyīyàng,yìkěyǐcóngxiàngliàngdejiāchéngxìngláikàn.\n",
            "1\t同樣\t同樣\tADV\tRB\t_\t12\tadvmod\t_\tSpaceAfter=No|Translit=tóngyàng|LTranslit=tóngyàng\n",
            "2\t，\t，\tPUNCT\t,\t_\t1\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "3\t施力\t施力\tVERB\tVV\t_\t5\tacl:relcl\t_\tSpaceAfter=No|Translit=shīlì|LTranslit=shīlì\n",
            "4\t的\t的\tSCONJ\tDEC\t_\t3\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "5\t大小\t大小\tNOUN\tNN\t_\t6\tnsubj\t_\tSpaceAfter=No|Translit=dàxiǎo|LTranslit=dàxiǎo\n",
            "6\t不同\t不同\tADJ\tJJ\t_\t8\tcsubj\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "7\t，\t，\tPUNCT\t,\t_\t6\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "8\t引起\t引起\tVERB\tVV\t_\t11\tacl:relcl\t_\tSpaceAfter=No|Translit=yǐnqǐ|LTranslit=yǐnqǐ\n",
            "9\t的\t的\tSCONJ\tDEC\t_\t8\tmark:rel\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "10\t加速\t加速\tVERB\tVV\t_\t11\tcompound\t_\tSpaceAfter=No|Translit=jiāsù|LTranslit=jiāsù\n",
            "11\t度\t度\tPART\tSFN\t_\t12\tnsubj\t_\tSpaceAfter=No|Translit=dù|LTranslit=dù\n",
            "12\t不同\t不同\tADJ\tJJ\t_\t0\troot\t_\tSpaceAfter=No|Translit=bùtóng|LTranslit=bùtóng\n",
            "13\t，\t，\tPUNCT\t,\t_\t29\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "14\t最終\t最終\tNOUN\tNN\t_\t16\tnmod\t_\tSpaceAfter=No|Translit=zuìzhōng|LTranslit=zuìzhōng\n",
            "15\t的\t的\tPART\tDEC\t_\t14\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "16\t結果\t結果\tNOUN\tNN\t_\t29\tnsubj\t_\tSpaceAfter=No|Translit=jiéguǒ|LTranslit=jiéguǒ\n",
            "17\t也\t也\tSCONJ\tRB\t_\t19\tmark\t_\tSpaceAfter=No|Translit=yě|LTranslit=yě\n",
            "18\t不\t不\tADV\tRB\t_\t19\tadvmod\t_\tSpaceAfter=No|Translit=bù|LTranslit=bù\n",
            "19\t一樣\t一樣\tADJ\tJJ\t_\t29\tadvcl\t_\tSpaceAfter=No|Translit=yīyàng|LTranslit=yīyàng\n",
            "20\t，\t，\tPUNCT\t,\t_\t19\tpunct\t_\tSpaceAfter=No|Translit=,|LTranslit=,\n",
            "21\t亦\t亦\tSCONJ\tRB\t_\t29\tmark\t_\tSpaceAfter=No|Translit=yì|LTranslit=yì\n",
            "22\t可以\t可以\tAUX\tMD\t_\t29\taux\t_\tSpaceAfter=No|Translit=kěyǐ|LTranslit=kěyǐ\n",
            "23\t從\t從\tADP\tIN\t_\t27\tcase\t_\tSpaceAfter=No|Translit=cóng|LTranslit=cóng\n",
            "24\t向量\t向量\tNOUN\tNN\t_\t27\tnmod\t_\tSpaceAfter=No|Translit=xiàngliàng|LTranslit=xiàngliàng\n",
            "25\t的\t的\tPART\tDEC\t_\t24\tcase\t_\tSpaceAfter=No|Translit=de|LTranslit=de\n",
            "26\t加成\t加成\tVERB\tVV\t_\t27\tcompound\t_\tSpaceAfter=No|Translit=jiāchéng|LTranslit=jiāchéng\n",
            "27\t性\t性\tPART\tSFN\t_\t29\tobl\t_\tSpaceAfter=No|Translit=xìng|LTranslit=xìng\n",
            "28\t來\t來\tSCONJ\tRB\t_\t29\tmark\t_\tSpaceAfter=No|Translit=lái|LTranslit=lái\n",
            "29\t看\t看\tVERB\tVV\t_\t12\tparataxis\t_\tSpaceAfter=No|Translit=kàn|LTranslit=kàn\n",
            "30\t。\t。\tPUNCT\t.\t_\t12\tpunct\t_\tSpaceAfter=No|Translit=.|LTranslit=.\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Model"
      ],
      "metadata": {
        "id": "4c1NFE-ak_F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMrTTIUqk-EP",
        "outputId": "7c68deff-5ec7-41db-ec61-dd2acfc41398"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizerFast\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import requests\n",
        "from conllu import parse"
      ],
      "metadata": {
        "id": "4n3vtYallaVi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"xlm-roberta-base\"\n",
        "\n",
        "# Download and parse the train dataset\n",
        "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-train.conllu\"\n",
        "\n",
        "dataset = download_and_parse(url)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "def align_tokens_and_labels(sentence, label_field='upos'):\n",
        "    tokens = [token['form'] for token in sentence]  # Original words from the sentence\n",
        "    labels = [token[label_field] for token in sentence]  # Corresponding labels\n",
        "\n",
        "    # Tokenize the sentence, indicating that the input is pre-tokenized\n",
        "    tokenized_input = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
        "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
        "\n",
        "    aligned_labels = []\n",
        "    # Iterate over word_ids to assign labels correctly\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            aligned_labels.append(-100)  # Assign -100 to special tokens\n",
        "        else:\n",
        "            # If the token starts with \"▁\" and is not just \"▁\", assign the label\n",
        "            if tokenized_input.tokens()[i].startswith(\"▁\") and len(tokenized_input.tokens()[i]) > 1:\n",
        "                aligned_labels.append(labels[word_idx])\n",
        "            elif not tokenized_input.tokens()[i].startswith(\"▁\"):\n",
        "                # Assign the label to the actual word token (not the underscore)\n",
        "                aligned_labels.append(labels[word_idx])\n",
        "            else:\n",
        "                # If it's just \"▁\", assign -100\n",
        "                aligned_labels.append(-100)\n",
        "\n",
        "    return tokenized_input, aligned_labels\n",
        "\n",
        "\n",
        "\n",
        "# Align tokens and labels for the first sentence as an example\n",
        "first_sentence = dataset[0]\n",
        "tokenized_input, aligned_labels = align_tokens_and_labels(first_sentence)\n",
        "\n",
        "\n",
        "print(first_sentence)\n",
        "print(\"Tokenized input:\", tokenized_input)\n",
        "print(\"Aligned labels:\", aligned_labels)\n",
        "\n",
        "# Convert token IDs back to tokens (words/subwords)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'][0])\n",
        "\n",
        "# Print each token with its aligned label\n",
        "for token, label in zip(tokens, aligned_labels):\n",
        "    print(f\"{token} --> {label}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9kIjYD2ldIx",
        "outputId": "53a45e55-fc43-475a-e3a9-9adef5ee61bb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TokenList<看似, 簡單, ，, 只, 是, 二, 選, 一, 做, 決擇, ，, 但, 其實, 他們, 代表, 的, 是, 你, 周遭, 的, 親朋, 好友, ，, 試, 著, 給, 你, 不同, 的, 意見, ，, 但, 追根究底, ，, 最後, 決定, 的, 還是, 自己, 。, metadata={sent_id: \"train-s1\", text: \"看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究底，最後決定的還是自己。\", translit: \"kànshìjiǎndān,zhǐshì'èrxuǎnyīzuòjuézé,dànqíshítāmendàibiǎodeshìnǐzhōuzāodeqīnpénghǎoyou,shìzhegěinǐbùtóngdeyìjiàn,dànzhuīgēnjiūdǐ,zuìhòujuédìngdeháishìzìjǐ.\"}>\n",
            "Tokenized input: {'input_ids': tensor([[     0,      6, 113875,      6,  32564,      6,      4,      6,   5344,\n",
            "              6,    354,  87744,      6,   6995,  45690,      6,   2213,      6,\n",
            "          33808, 235211,      6,      4,  53072,      6,  16827,      6,   8056,\n",
            "              6,   6959,      6,     43,      6,    354,  73675,      6,   6271,\n",
            "          44162,      6,     43,      6,  11638, 182529,      6,  81070,      6,\n",
            "              4,      6,  12324,      6,   3094,      6,   5862,  73675,      6,\n",
            "           5714,      6,     43,      6,  31505,      6,      4,  53072,      6,\n",
            "          17411,  12442,  78382,  11298,      6,      4,      6,  15394,      6,\n",
            "          17755,      6,     43,      6,  11097,      6,   1652,      6,     30,\n",
            "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Aligned labels: [-100, -100, 'VERB', -100, 'ADJ', -100, 'PUNCT', -100, 'ADV', -100, 'VERB', 'NUM', -100, 'VERB', 'NUM', -100, 'VERB', -100, 'NOUN', 'NOUN', -100, 'PUNCT', 'SCONJ', -100, 'ADV', -100, 'PRON', -100, 'VERB', -100, 'SCONJ', -100, 'AUX', 'PRON', -100, 'NOUN', 'NOUN', -100, 'PART', -100, 'NOUN', 'NOUN', -100, 'NOUN', -100, 'PUNCT', -100, 'VERB', -100, 'AUX', -100, 'VERB', 'PRON', -100, 'ADJ', -100, 'SCONJ', -100, 'NOUN', -100, 'PUNCT', 'SCONJ', -100, 'VERB', 'VERB', 'VERB', 'VERB', -100, 'PUNCT', -100, 'NOUN', -100, 'VERB', -100, 'SCONJ', -100, 'AUX', -100, 'PRON', -100, 'PUNCT', -100]\n",
            "<s> --> -100\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "看似 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "簡單 --> ADJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "只 --> ADV\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "是 --> VERB\n",
            "\n",
            "▁二 --> NUM\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "選 --> VERB\n",
            "\n",
            "▁一 --> NUM\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "做 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "決 --> NOUN\n",
            "\n",
            "擇 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁但 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "其實 --> ADV\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "他們 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "代表 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "是 --> AUX\n",
            "\n",
            "▁你 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "周 --> NOUN\n",
            "\n",
            "遭 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> PART\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "親 --> NOUN\n",
            "\n",
            "朋 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "好友 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "試 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "著 --> AUX\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "給 --> VERB\n",
            "\n",
            "▁你 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "不同 --> ADJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "意見 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁但 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "追 --> VERB\n",
            "\n",
            "根 --> VERB\n",
            "\n",
            "究 --> VERB\n",
            "\n",
            "底 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            ", --> PUNCT\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "最後 --> NOUN\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "決定 --> VERB\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "的 --> SCONJ\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "還是 --> AUX\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "自己 --> PRON\n",
            "\n",
            "▁ --> -100\n",
            "\n",
            "。 --> PUNCT\n",
            "\n",
            "</s> --> -100\n",
            "\n"
          ]
        }
      ]
    }
  ]
}